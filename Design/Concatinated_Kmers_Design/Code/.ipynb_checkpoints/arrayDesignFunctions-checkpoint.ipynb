{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from jellyfish import hamming_distance\n",
    "from Bio.Seq import reverse_complement\n",
    "\n",
    "def testDuplicates(df, colname, processName, dfName):\n",
    "    \"\"\"\n",
    "    Given a pandas dataframe, column name, and name of the process, if there are duplicate\n",
    "        entires, an error is raised and the duplicated entries are listed.\n",
    "    \"\"\"\n",
    "    dupRows = df[df[colname].duplicated()]\n",
    "    if len(dupRows) > 0:\n",
    "        dupEntries = list(set(dupRows[colname]))\n",
    "        raise ValueError((f\"During {processName} in {dfName}, the following entires\"\n",
    "                         f\"had duplicates:\\n{dupEntries}\"))\n",
    "\n",
    "def testProbeLen(probe, reqLen, funName):\n",
    "    \"\"\"\n",
    "    Tests if a probe is the required length, if not raises a value error. \n",
    "    \"\"\"\n",
    "    probeLen = len(probe)\n",
    "    if probeLen != reqLen:\n",
    "        raise ValueError(f\"In function {funName}, probe length should be {reqLen}, len = {probeLen}\")\n",
    "    \n",
    "    \n",
    "def dinucMutation(seq, r, description, dinucList, cons = False):\n",
    "    \"\"\"\n",
    "    Given a sequence, seq, generate all possible dinucleotide mutations\n",
    "    1. seq = sequence to mutate\n",
    "    2. r = range in sequence to mutate, given as a list or tuple\n",
    "     ex/ r = [0, 5] or (4,9)\n",
    "    3. dinucList = a precomputed list of all dinucleotides. \n",
    "    4. cons = consolodate. Argument to consolodate matching mutations.\n",
    "    \"\"\"\n",
    "    # Generate all sequences with mutations at given ranges\n",
    "    mutList, mutDescList = [], []\n",
    "    for i in range(r[0], r[1]):\n",
    "        for mut in dinucList:\n",
    "            mutation = seq[:i] + mut + seq[i + 2:]\n",
    "            testProbeLen(mutation, 25, \"dinucMutation\")\n",
    "            mutDesc = f\"DN_{description}_pos{i}->{mut}\"\n",
    "            mutList.append(mutation)\n",
    "            mutDescList.append(mutDesc)\n",
    "    mutDF = pd.DataFrame({\"Sequence\":mutList, \"Description\":mutDescList})\n",
    "    # Filter out mutations that match the original\n",
    "    mutDF = mutDF.query(\"Sequence != @seq\").reset_index(drop=True)\n",
    "    # Consolodate equivalent mutations, if cons argument is True\n",
    "    if cons == True:\n",
    "        mutList, descList  = [], []\n",
    "        for mutSeq, group in mutDF.groupby(by=\"Sequence\"):\n",
    "            consDesc = '-'.join(group[\"Description\"])\n",
    "            mutList.append(mutSeq)\n",
    "            descList.append(consDesc)\n",
    "        mutDF = pd.DataFrame({\"Sequence\":mutList, \"Description\":descList})\n",
    "    # Append the original sequence\n",
    "    mutDF = mutDF.append({\"Sequence\":seq, \"Description\":f\"DN_{description}_posW->WT\"}, ignore_index = True)\n",
    "    return(mutDF)\n",
    "\n",
    "def diFlankMutation(seq, coreRange, description, tetranucList):\n",
    "    \"\"\"\n",
    "    Given a sequence, seq, generate all possible dinucleotide flanks\n",
    "    1. seq = sequence to mutate\n",
    "    2. coreRange = range in sequence to mutate, given as a list or tuple\n",
    "     ex/ r = [0, 5] or (4,9)\n",
    "    3. dinucList = a precomputed list of all dinucleotides. \n",
    "    4. cons = consolodate. Argument to consolodate matching mutations.    \n",
    "    \"\"\"\n",
    "    mutList, mutDescList = [], []\n",
    "    cFlankLeft = seq[0:coreRange[0] - 2]\n",
    "    cFlankRight = seq[coreRange[1] + 2:]\n",
    "    core = seq[coreRange[0]:coreRange[1]]\n",
    "    for tetramer in tetranucList:\n",
    "        mut = cFlankLeft + tetramer[:2] + core + tetramer[2:] + cFlankRight\n",
    "        testProbeLen(mut, 25, \"diFlankMutation\")\n",
    "        mutList.append(mut)\n",
    "        mutDescList.append(f\"Flank_{description}_{tetramer}\")\n",
    "    return(pd.DataFrame({\"Sequence\":mutList, \"Description\":mutDescList}))\n",
    "\n",
    "\n",
    "def addReps(df, nReps):\n",
    "    \"\"\"\n",
    "    Adds replicates to a dataframe of sequences\n",
    "    df = Pandas DataFrame of sequences to generate replicates of\n",
    "     must have a \"Sequence\" and \"Description\" column\n",
    "    nReps = number of replicates, given as an integer\n",
    "    \"\"\"\n",
    "    seqList, desList = [], []\n",
    "    for seq, des in zip(df[\"Sequence\"], df[\"Description\"]):\n",
    "        for i in range(nReps):\n",
    "            seqList.append(seq)\n",
    "            desList.append(f\"{des}_r{i}\")\n",
    "    return(pd.DataFrame({\"Sequence\":seqList, \"Description\":desList}))\n",
    "\n",
    "def sumScore(seq, k, kScoreDict):\n",
    "    \"\"\"\n",
    "    Given a sequence and a dictionary of scores for each kmer of length k,\n",
    "        returns the sum of those scores for all substrings in the sequence\n",
    "        of length k.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        score = score + kScoreDict[seq[i:i+k]]\n",
    "    return(score)\n",
    "\n",
    "def scoreClusters(df, kDict):\n",
    "    \"\"\"\n",
    "    Given a pandas dataframe of clustered sequences, applies the sumScore\n",
    "        function and returns a dataframe with only the top scoring sequence\n",
    "        per cluster.\n",
    "    \"\"\"\n",
    "    df[\"Score\"] = df[\"Sequence\"].apply(lambda x: sumScore(x, 6, kDict))\n",
    "    df = df.sort_values(by = \"Score\", ascending = False)\n",
    "    df = df.drop_duplicates(subset=\"Cluster\")\n",
    "    return(df)\n",
    "\n",
    "def closestSubHamming(string, query):\n",
    "    \"\"\"\n",
    "    Given a string and a query string that is the length of the string or\n",
    "        less, searches all substrings the length of the query. Returns the\n",
    "        smallest hamming distance across all substrings. \n",
    "    \"\"\"\n",
    "    qLen = len(query)\n",
    "    minHam = qLen\n",
    "    for i in range(len(string) - qLen + 1):\n",
    "        ham = hamming_distance(query, string[i:i+qLen])\n",
    "        if ham < minHam:\n",
    "            minHam = ham\n",
    "    return(minHam)\n",
    "\n",
    "def reverseComplementDF(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe with a sequence and description\n",
    "     ensures the sequence is upper case and applies the reverse_complement \n",
    "     function to the sequence. Returns a pandas dataframe with the reverse \n",
    "     complement sequences.\n",
    "    \"\"\"\n",
    "    dfOut = df.copy(deep = True)\n",
    "    seqList, descList  = [], []\n",
    "    for seq, desc in zip(dfOut[\"Sequence\"], dfOut[\"Description\"]):\n",
    "        seqList.append(reverse_complement(seq.upper()))\n",
    "        descList.append(f\"{desc}_O2\")\n",
    "    dfOut[\"Sequence\"] = seqList\n",
    "    dfOut[\"Description\"] = descList\n",
    "    return(dfOut)\n",
    "\n",
    "def removeRepString(string):\n",
    "    \"\"\"\n",
    "    Given a string with replicate information at the end seperated by an underscore,\n",
    "        returns a string without the replicate information.\n",
    "    \"\"\"\n",
    "    strList = string.split(\"_\")[:-1]\n",
    "    return(\"_\".join(strList))\n",
    "\n",
    "def createDiProbes(inputFileLoc, primer, dinucList):\n",
    "    \"\"\"\n",
    "    Given a sequence design file for dinucleotide mutations and primer sequence, \n",
    "    returns a pandas DataFrame of the probes. \n",
    "    \"\"\"\n",
    "    # Read file and ensure sequences are upper case\n",
    "    designDiDF = pd.read_csv(inputFileLoc)\n",
    "    testDuplicates(designDiDF, \"Description\", \"createDiProbes\", \"designDiDF\")\n",
    "    designDiDF[\"Sequence\"] = designDiDF[\"Sequence\"].apply(lambda x: x.upper())\n",
    "    # Generate Dataframes for each sequence, concatinate them\n",
    "    diMutDFList = []\n",
    "    for desc, seq, mutSubStr in zip(designDiDF[\"Description\"],\n",
    "                                    designDiDF[\"Sequence\"],\n",
    "                                    designDiDF[\"Mutation_Substring\"]):\n",
    "        rStart = seq.find(mutSubStr)\n",
    "        rEnd = rStart + len(mutSubStr)\n",
    "        diMutDFList.append(dinucMutation(seq, [rStart, rEnd],desc,dinucList, cons=True))\n",
    "    diMutDF = pd.concat(diMutDFList)\n",
    "    # Add Primer to the sequences\n",
    "    diMutDF[\"Sequence\"] = diMutDF[\"Sequence\"].apply(lambda x: x + primer)\n",
    "    # Make 20 replicates for each sequence\n",
    "    diMutDF = addReps(diMutDF, 20)\n",
    "    return(diMutDF)\n",
    "\n",
    "def createFlankProbes(inputFileLoc, primer, tetramerList):\n",
    "    \"\"\"\n",
    "    Given an input file of sequences with a core sequence defined,\n",
    "        the primer, and a list of all tetramers, returns a pandas dataframe with\n",
    "        all possible 2-mer flanks on each side of the core sequence.\n",
    "    \"\"\"\n",
    "    designFlankDF = pd.read_csv(inputFileLoc)\n",
    "    testDuplicates(designFlankDF, \"Description\", \"createFlankProbes\", \"designFlankDF\")\n",
    "    designFlankDF[\"Sequence\"] = designFlankDF[\"Sequence\"].apply(lambda x: x.upper())\n",
    "    # Generate Dataframes for each sequence, concatinate them\n",
    "    flankDFList = []\n",
    "    for desc, seq, coreSubStr in zip(designFlankDF[\"Description\"],\n",
    "                                     designFlankDF[\"Sequence\"],\n",
    "                                     designFlankDF[\"Core_Sequence\"]):\n",
    "        rStart = seq.find(coreSubStr)\n",
    "        rEnd = rStart + len(coreSubStr)\n",
    "        flankDFList.append(diFlankMutation(seq, [rStart, rEnd], desc, tetramerList))\n",
    "    flankDF = pd.concat(flankDFList)\n",
    "    # Add Primer to the sequences\n",
    "    flankDF[\"Sequence\"] = flankDF[\"Sequence\"].apply(lambda x: x + primer)\n",
    "    # Make 20 replicates for each sequence\n",
    "    flankDF = addReps(flankDF, 20)\n",
    "    return(flankDF)\n",
    "\n",
    "def extractDeBruijn(inputFileLoc):\n",
    "    \"\"\"\n",
    "    Given an array design file, extracts all de Bruijn sequences and returns a pandas\n",
    "        dataframe with those sequences.\n",
    "    \"\"\"\n",
    "    # Read in a previous array design with the de Bruijn sequences and same primer\n",
    "    arrayDesignDF = pd.read_csv(inputFileLoc, sep = '\\t')\n",
    "    # Filter the dataframe for the de Bruijn sequences\n",
    "    arrayDesignDF = arrayDesignDF.dropna(subset=[\"Sequence\"])\n",
    "    deBruDF = arrayDesignDF[arrayDesignDF[\"Name\"].str.startswith(\"All_9mer_14bp\") |\n",
    "                           arrayDesignDF[\"Name\"].str.startswith(\"All_8mer_13bp\")].copy(deep = True)\n",
    "    # Rename columns\n",
    "    deBruDF = deBruDF[[\"Sequence\", \"Name\"]]\n",
    "    deBruDF = deBruDF.rename(columns={\"Name\":\"Description\"})\n",
    "    # Test that the description has no duplicates, all sequences are 60bp\n",
    "    testDuplicates(deBruDF, \"Description\", \"extractDeBruijn\", \"deBruDF\")\n",
    "    deBruDF[\"Sequence\"].apply(lambda x: testProbeLen(x, 60, \"extractDeBruijn\") )\n",
    "    return(deBruDF) \n",
    "\n",
    "def createKDProbes(inputFileLoc, primer):\n",
    "    \"\"\"\n",
    "    Given a file with kD probes, adds 20 replicates to each sequence and returns a \n",
    "        pandas dataframe\n",
    "    \"\"\"\n",
    "    kDDF = pd.read_csv(inputFileLoc)\n",
    "    # Check that there are no duplicates in the description and sequences are 25bp\n",
    "    testDuplicates(kDDF, \"Description\", \"createKDProbes\", \"kDDF\")\n",
    "    kDDF[\"Sequence\"].apply(lambda x: testProbeLen(x, 25, \"createKDProbes\"))\n",
    "    # Make a pandas dataframe with the reverse complement sequences\n",
    "    kDDFRC = reverseComplementDF(kDDF)\n",
    "    # Add primer to sequences\n",
    "    kDDF[\"Sequence\"] = kDDF[\"Sequence\"].apply(lambda x: x + primer)\n",
    "    kDDFRC[\"Sequence\"] = kDDFRC[\"Sequence\"].apply(lambda x: x + primer)\n",
    "    # Concatinate both dataframes, prepend kD_ to the description, add replicates\n",
    "    kDDF = pd.concat([kDDF, kDDFRC])\n",
    "    kDDF[\"Description\"] = kDDF[\"Description\"].apply(lambda x: f\"kD_{x}\")\n",
    "    kDDF = addReps(kDDF, 20)\n",
    "    return(kDDF)\n",
    "\n",
    "def createClustDiffProbes(proteinName, kmerFileLoc, clusterFileLoc, extension):\n",
    "    \"\"\"\n",
    "    Given a protein name, kmer file of Escores for UV and Watson-Crick data, a cluster file,\n",
    "        and an extension label, returns the top sequence per cluster with a description of the method for\n",
    "        generating the sequence.\n",
    "    \"\"\"\n",
    "    # Read in kmer file and cluster file\n",
    "    kmerDF = pd.read_csv(kmerFileLoc, sep = '\\t')\n",
    "    clustDF = pd.read_csv(clusterFileLoc, sep = '\\t', skiprows=2)\n",
    "    # Calculate differences in UV vs Watson-Crick Escores\n",
    "    kmerDF[\"Diff\"] = kmerDF[\"Escore_UV9\"] - kmerDF[\"Escore_WC8\"]\n",
    "    # Relate the difference to kmers as a dictionary\n",
    "    kDiffDict = dict(zip(kmerDF[\"kmerFwd\"], kmerDF[\"Diff\"]))\n",
    "    kDiffDict.update(dict(zip(kmerDF[\"kmerRC\"], kmerDF[\"Diff\"])))\n",
    "    # Get the top sequence per cluster, using the kmer difference dictionary\n",
    "    topSeqPerCluster = scoreClusters(clustDF, kDiffDict)\n",
    "    topSeqPerCluster[\"Description\"] = topSeqPerCluster[\"Cluster\"].apply(lambda x: f\"Clst_{proteinName}_C{x}_{extension}\")\n",
    "    return(topSeqPerCluster)\n",
    "\n",
    "def createClustCloseProbes(proteinName, kmerFileLoc, clusterFileLoc, extension, seq, nProbes):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "        1. protein name\n",
    "        2. Location of the kmer file with UV and Watson-Crick measurements\n",
    "        3. Location of the cluster file\n",
    "        4. Extension, indicating the method used to generate the sequence\n",
    "        5. Sequence to find the closest hamming distance of a substring in the clustered file.\n",
    "        6. Number of probes to generate\n",
    "    Returns a pandas dataframe with the sequences that have the smallest hamming distance the length\n",
    "        of the query sequence\n",
    "    \"\"\"\n",
    "    # Read in kmer file and cluster file\n",
    "    kmerDF = pd.read_csv(kmerFileLoc, sep = '\\t')\n",
    "    clustDF = pd.read_csv(clusterFileLoc, sep = '\\t', skiprows=2)\n",
    "    # For each sequence, find the closest substring hamming distance\n",
    "    hDistList = []\n",
    "    for i in clustDF[\"Sequence\"]:\n",
    "        # Distance is the minimum of either orientation\n",
    "        hDistFwd = closestSubHamming(i, seq)\n",
    "        hDistRC = closestSubHamming(i, reverse_complement(seq))\n",
    "        hDistList.append(min([hDistFwd, hDistRC]))\n",
    "    clustDF[\"hDist\"] = hDistList\n",
    "    clustDF = clustDF.sort_values(by = \"hDist\").head(n = nProbes).reset_index(drop=True)\n",
    "    desc = []\n",
    "    for idx, hDist in enumerate(clustDF[\"hDist\"]):\n",
    "        desc.append(f\"ClstD_{proteinName}_H{hDist}_{extension}_S{idx}\")\n",
    "    clustDF[\"Description\"] = desc\n",
    "    clustDF = clustDF[[\"Sequence\", \"Description\"]]\n",
    "    testDuplicates(clustDF, \"Sequence\", \"createClustCloseProbes\", \"clustDF\")\n",
    "    testDuplicates(clustDF, \"Description\", \"createClustCloseProbes\", \"clustDF\")\n",
    "    return(clustDF)    \n",
    "\n",
    "def createClustDiffProbesFromLists(proteinNameList, \n",
    "                                  kmerFileLocList, \n",
    "                                  clusterKmersFileLocList,\n",
    "                                  clusterShortExtFileLocList,\n",
    "                                  primer,\n",
    "                                  replicate_number):\n",
    "    \"\"\"\n",
    "    Wrapper function for createClustDiffProbes to generate cluster probes for the array.\n",
    "    Given a list of protein names, kmer file locations, and cluster file locations\n",
    "     for both methods, a sequence to compare hamming distances, and the number of\n",
    "     probes to generate for clost probes to that sequence. \n",
    "    Returns a pandas Dataframe with the probes. \n",
    "    \"\"\"\n",
    "    dfList = []\n",
    "    # Process kmer clusters\n",
    "    for protein, kmerFile, clusterKmer in zip(proteinNameList,\n",
    "                                              kmerFileLocList,\n",
    "                                              clusterKmersFileLocList):\n",
    "        clusterDF = createClustDiffProbes(protein, kmerFile, clusterKmer, \"TD25\")\n",
    "        dfList.append(clusterDF)\n",
    "    # Process extended short clusters\n",
    "    for protein, kmerFile, clusterKmer in zip(proteinNameList,\n",
    "                                              kmerFileLocList,\n",
    "                                              clusterShortExtFileLocList):\n",
    "        clusterDF = createClustDiffProbes(protein, kmerFile, clusterKmer, \"EXT\")\n",
    "        dfList.append(clusterDF)\n",
    "    clusterProbeDF = pd.concat(dfList)\n",
    "    clusterProbeDF = clusterProbeDF.reset_index(drop = True)\n",
    "    clusterProbeRCDF = reverseComplementDF(clusterProbeDF)\n",
    "    clusterProbeDF[\"Sequence\"] = clusterProbeDF[\"Sequence\"].apply(lambda x: x + primer)\n",
    "    clusterProbeRCDF[\"Sequence\"] = clusterProbeRCDF[\"Sequence\"].apply(lambda x: x + primer)\n",
    "    clusterProbeDF = addReps(clusterProbeDF, replicate_number)\n",
    "    clusterProbeRCDF = addReps(clusterProbeRCDF, replicate_number)\n",
    "    return(pd.concat([clusterProbeDF,clusterProbeRCDF]))\n",
    "\n",
    "def createClustCloseProbesFromLists(proteinNameList, \n",
    "                                  kmerFileLocList, \n",
    "                                  clusterKmersFileLocList,\n",
    "                                  clusterShortExtFileLocList,\n",
    "                                  sequence,\n",
    "                                  nProbes,\n",
    "                                  primer,\n",
    "                                  replicate_number):\n",
    "    \"\"\"\n",
    "    Wrapper function for createClustCloseProbes. \n",
    "    \"\"\"\n",
    "    dfList = []\n",
    "    # Process kmer clusters\n",
    "    for protein, kmerFile, clusterKmer in zip(proteinNameList,\n",
    "                                              kmerFileLocList,\n",
    "                                              clusterKmersFileLocList):\n",
    "        clusterDF = createClustCloseProbes(protein, kmerFile, clusterKmer, \"TD25\", sequence, nProbes)\n",
    "        dfList.append(clusterDF)\n",
    "    # Process extended short clusters\n",
    "    for protein, kmerFile, clusterKmer in zip(proteinNameList,\n",
    "                                              kmerFileLocList,\n",
    "                                              clusterShortExtFileLocList):\n",
    "        clusterDF = createClustCloseProbes(protein, kmerFile, clusterKmer, \"EXT\", sequence, nProbes)\n",
    "        dfList.append(clusterDF)\n",
    "    clusterProbeDF = pd.concat(dfList)\n",
    "    clusterProbeDF = clusterProbeDF.reset_index(drop = True)\n",
    "    clusterProbeRCDF = reverseComplementDF(clusterProbeDF)\n",
    "    clusterProbeDF[\"Sequence\"] = clusterProbeDF[\"Sequence\"].apply(lambda x: x + primer)\n",
    "    clusterProbeRCDF[\"Sequence\"] = clusterProbeRCDF[\"Sequence\"].apply(lambda x: x + primer)\n",
    "    clusterProbeDF = addReps(clusterProbeDF, replicate_number)\n",
    "    clusterProbeRCDF = addReps(clusterProbeRCDF, replicate_number)\n",
    "    return(pd.concat([clusterProbeDF,clusterProbeRCDF]))   \n",
    "\n",
    "def positionalProbes(desc, insert, baseSeq):\n",
    "    \"\"\"\n",
    "    Given a description of the probe, an insert sequence to move across positions, and a base sequence\n",
    "        that the insert sequence is inserted into, returns a pandas dataframe with the insert sequences at\n",
    "        every second position and at the start of the probe that is 59 bases long as a special case. \n",
    "    \"\"\"\n",
    "    probeList, descList = [], []\n",
    "    for i in range(int(((len(baseSeq) - len(insert) + 1)))):\n",
    "        if i % 2 == 0:\n",
    "            probe = baseSeq[:i] + insert + baseSeq[i + len(insert):]\n",
    "            probeLen = len(probe)\n",
    "            testProbeLen(probe, 25, \"positionalProbes\")\n",
    "            probeList.append(probe)\n",
    "    for i in range(len(probeList)):\n",
    "        descList.append(f\"Pos_{desc}_P{i * 2}\")\n",
    "    # Append shortened probe with 0 position to list\n",
    "    shortProbe = insert[1:] + baseSeq[len(insert):]\n",
    "    shortProbeLen = len(shortProbe)\n",
    "    testProbeLen(shortProbe, 24, \"positionalProbes\")\n",
    "    probeList.append(shortProbe)\n",
    "    descList.append(f\"Pos_{desc}_P0_Short\")\n",
    "    return(pd.DataFrame({\"Sequence\":probeList, \"Description\":descList}))\n",
    "\n",
    "def createPositionalProbes(positionalProbeFile, primer):\n",
    "    \"\"\"\n",
    "    Wrapper for positional probes\n",
    "    \"\"\"\n",
    "    posProbeInputDF = pd.read_csv(positionalProbeFile)\n",
    "    positionalProbeDFList = []\n",
    "    for desc, insert, baseSeq, in zip(posProbeInputDF[\"Description\"],\n",
    "                                      posProbeInputDF[\"Sequence_Insert\"],\n",
    "                                      posProbeInputDF[\"Base_Sequence\"]):\n",
    "        positionalProbeDFList.append(positionalProbes(desc, insert, baseSeq))\n",
    "    posProbeDF = pd.concat(positionalProbeDFList)\n",
    "    posProbeDF = posProbeDF.reset_index(drop=True)\n",
    "    posProbeDF = addReps(posProbeDF, 20)\n",
    "    posProbeDF[\"Sequence\"] = posProbeDF[\"Sequence\"].apply(lambda x: x + primer)\n",
    "    return(posProbeDF)\n",
    "\n",
    "def saveUniqueProbes(probeDF,outputFileLoc = \"../Array_Probes.txt\"):\n",
    "    \"\"\"\n",
    "    Makes a copy of the array design without replicates and saves it to a file.\n",
    "    \"\"\"\n",
    "    # Generate a copy of the array without replicates\n",
    "    uvArrayUnique = probeDF.copy(deep = True)\n",
    "    uvArrayUnique[\"Description\"] = uvArrayUnique[\"Description\"].apply(lambda x: removeRepString(x))\n",
    "    uvArrayUnique = uvArrayUnique.drop_duplicates()\n",
    "    uvArrayUnique.to_csv(outputFileLoc, sep = '\\t', header = None, index = False)\n",
    "\n",
    "def saveAgilentFormattedArray(probeDFInput, outputFileLoc = \"../Array_Design_Full.txt\"):\n",
    "    \"\"\"\n",
    "    Formats a dataframe of probes with a sequence and description in a format that is compatible with\n",
    "        the format sent to Agilent to order custom microarrays. Saves this as a tab-seperated file.\n",
    "    \"\"\"\n",
    "    probeDF = probeDFInput.copy(deep = True)\n",
    "    # Generate UV Array file formated for ordering form Agilent\n",
    "    IDList = []\n",
    "    for idx, row in enumerate(probeDF['Sequence']):\n",
    "        ID = f\"Ctrl_UVT_{str(idx).zfill(6)}\"\n",
    "        if len(ID) > 15:\n",
    "            raise ValueError(f\"ProbeID {ID} is {len(ID)} characters and needs to be 15 or less\")\n",
    "        IDList.append(ID)\n",
    "    probeDF['ID'] = IDList\n",
    "    probeDF = probeDF[['ID', 'Sequence', 'Description']]\n",
    "    probeDF['NA'] = 'NA|NA'\n",
    "    probeDF['NA2'] = 'NA'\n",
    "    probeDF['Desc2'] = probeDF['Description']\n",
    "    probeDF['chr'] = 'chr1:0-0'\n",
    "    probeDF.to_csv(outputFileLoc, sep = '\\t', header = None, index = False)\n",
    "\n",
    "def testProbeLenMatchDesc(probe, description):\n",
    "    \"\"\"\n",
    "    Tests a probe to see if it is appropriate for the associated description.\n",
    "    \"\"\"\n",
    "    if len(probe) != 60 and \"Short\" not in description:\n",
    "        raise ValueError(f\"Probe length should be 60, len = {len(i)}\\n{i}\")\n",
    "    if len(probe) != 59 and \"Short\" in description:\n",
    "        raise ValueError(f\"Probe length should be 59, len = {len(i)}\\n{i}\")\n",
    "\n",
    "                         \n",
    "def testProbePrimer(probe, primer):\n",
    "    \"\"\"\n",
    "    Tests a probe to see if it contains the primer sequence.\n",
    "    \"\"\"\n",
    "    if primer not in probe:\n",
    "        raise ValueError(f\"Primer not found in sequence: {probe}\")\n",
    "\n",
    "    \n",
    "def testDNDescription(probe, description):\n",
    "    \"\"\"\n",
    "    For dinucleotide tests, parses the description for the probe to see if it\n",
    "        matches the actual sequence. \n",
    "    \"\"\"\n",
    "    descList = description.split(\"_\")\n",
    "    posInfoList = descList[3].split(\"->\")\n",
    "    dnPos = posInfoList[0][3:]\n",
    "    if dnPos.isnumeric():\n",
    "        dnPos = int(dnPos)\n",
    "        dnMut = posInfoList[1]\n",
    "        if probe[dnPos:dnPos + 2] != dnMut:\n",
    "            raise ValueError(f\"In probe: {probe}\\nthe mutation {dnMut} was not found at position {dnPos}\")\n",
    "    elif dnPos != \"W\":\n",
    "        raise ValueError(f\"In probe: {probe}\\ndescription: {description}\\n the postion could not be parsed.\")\n",
    "\n",
    "def testMultiDNDescription(probe, description):\n",
    "    \"\"\"\n",
    "    Checks that there is a maximum of 2 description per probe. If so, \n",
    "        applies testDNDescription to each description for a probe.\n",
    "    \"\"\"\n",
    "    if description[:2] == \"DN\":\n",
    "        descriptionList = description.split(\"-D\")\n",
    "        nDesc = len(descriptionList)\n",
    "        if  nDesc > 2:\n",
    "            raise ValueError(f\"There are {nDesc} description for DN probe {probe}\")\n",
    "        for desc in descriptionList:\n",
    "            testDNDescription(probe, desc)\n",
    "\n",
    "def testFlankingProbe(probe, description):\n",
    "    \"\"\"\n",
    "    Given a probe for the all possible 2-mer flanks assay, tests that the description\n",
    "        matches the probe.\n",
    "    \"\"\"\n",
    "    if description[:4] == \"Flank\":\n",
    "        descList = description.split(\"_\")\n",
    "        querySubstr = descList[3][:2] + descList[2] + descList[3][2:]\n",
    "        if querySubstr not in probe:\n",
    "            raise ValueError(f\"In flanking probe: {probe}\\nThe substring {querySubstr} was not found.\")\n",
    "\n",
    "\n",
    "def testDeBruijnProbes(arrayDF, arrayInfoList):\n",
    "    \"\"\"\n",
    "    Checks that all UV de Bruijn probes are shared across array design files. If so, checks if the shared\n",
    "        set is contained within the array design\n",
    "    \"\"\"\n",
    "    # Check that all UV de Bruijn probes are shared across array design files\n",
    "    arrayInfoDF = extractDeBruijn(f\"../Sequence_Designs/{arrayInfoList[0]}\")\n",
    "    arrayLen = len(arrayInfoDF)\n",
    "    for arrayInfoFile in arrayInfoList[1:]:\n",
    "        newArray = extractDeBruijn(f\"../Sequence_Designs/{arrayInfoFile}\")\n",
    "        arrayInfoDF = pd.merge(arrayInfoDF, newArray)\n",
    "        if len(arrayInfoDF) != arrayLen:\n",
    "            raise ValueError(f\"Array design file {arrayInfoFile} has a different set of de Bruijn sequences\")\n",
    "    # Check that all are present in the array design\n",
    "    if arrayLen != len(pd.merge(arrayDF, arrayInfoDF)):\n",
    "        raise ValueError(f\"UV array is missing {len(arrayInfoDF) - len(arrayDF)} de bruijn sequences\")\n",
    "        \n",
    "def testPosProbes(df, posProbeFile):\n",
    "    \"\"\"\n",
    "    Checks that the positional probe descriptions match the sequence. \n",
    "    \"\"\"\n",
    "    posDF = pd.read_csv(posProbeFile)\n",
    "    posDict = dict(zip(posDF[\"Description\"], posDF[\"Sequence_Insert\"]))\n",
    "    for TF in posDict.keys():\n",
    "        qDF = df[df[\"Description\"].str.startswith(f\"Pos_{TF}\")].copy(deep = True)\n",
    "        qDF[\"Pos\"] = qDF[\"Description\"].apply(lambda x: int(x.split('_')[3][1:]))\n",
    "        # Test that the insert sequence position in the description matches \n",
    "        for pos, seq in zip(qDF[\"Pos\"], qDF[\"Sequence\"]):\n",
    "            if len(seq) == 60:\n",
    "                qSeq = posDict[TF]\n",
    "            elif len(seq) == 59:\n",
    "                qSeq = posDict[TF][1:]\n",
    "            else:\n",
    "                raise ValueError(f\"Sequence {seq} is not 59bp or 60bp long\")\n",
    "            if seq[pos:pos+len(qSeq)] != qSeq:\n",
    "                raise ValueError(f\"Sequence:{qSeq} for TF {TF}\\nshould be at position {pos} in\\n{seq}\")\n",
    "        # Test that positions given are in sets of 2\n",
    "        uniquePos = list(set(qDF[\"Pos\"]))\n",
    "        for i in uniquePos:\n",
    "            if i % 2 != 0:\n",
    "                raise ValueError(f\"An odd numbered position, {i}, was used in a positional assay design.\")\n",
    "\n",
    "def testClusterDescription(df):\n",
    "    clustO1 = df[(df[\"Description\"].str.startswith(\"Clst_\")) &\n",
    "                     (~df[\"Description\"].str.contains(\"_O2_\"))].copy(deep = True).reset_index(drop=True)\n",
    "    # Test that all clusters are represented\n",
    "    clustO1[[\"TF\", \"Cluster\", \"Assay\"]] = clustO1[\"Description\"].apply(lambda x: pd.Series(x.split(\"_\")[1:4]))\n",
    "    clustO1[\"Cluster\"] = clustO1[\"Cluster\"].apply(lambda x: int(x[1:]))\n",
    "    assayDict = {\"TD25\":\"25mers\", \"EXT\":\"extShort\"}\n",
    "    for nameTup, groupDF in clustO1.groupby(by=[\"TF\", \"Assay\"]):\n",
    "        cMin = min(groupDF[\"Cluster\"])\n",
    "        cMax = max(groupDF[\"Cluster\"])\n",
    "        cSet = set(groupDF[\"Cluster\"])\n",
    "        if cMin != 0:\n",
    "            raise ValueError(f\"Minimum cluster for {nameTup[0]} with method {nameTup[1]} should be 0, not {cMin}\")\n",
    "        for i in range(cMin, cMax):\n",
    "            if i not in cSet:\n",
    "                raise ValueError(f\"Cluster {i} is missing from {nameTup[0]}, method {nameTup[1]}\")\n",
    "        # Test that sequence belongs to the cluster in the original file and is the max value\n",
    "        clusterDF = pd.read_csv(f\"../Data/Diff_UV_Probes/{nameTup[0]}_{assayDict[nameTup[1]]}_cluster.tsv\",\n",
    "                    sep = '\\t', skiprows=2)\n",
    "        testDuplicates(clusterDF, \"Sequence\", \"Cluster\", f\"{nameTup}\")\n",
    "        clusterDict = dict(zip(clusterDF[\"Sequence\"], clusterDF[\"Cluster\"]))\n",
    "        for seq, cluster in zip(groupDF[\"Sequence\"], groupDF[\"Cluster\"]):\n",
    "            if cluster != clusterDict[seq[:25]]:\n",
    "                raise ValueError((f\"Sequence {seq} was described as being in cluster {cluster},\" \n",
    "                                  \"found to be in {clusterDict[seq[:25]]}\"))\n",
    "    # Test that for each cluster in O1 there is an O2\n",
    "    clustO2 = df[(df[\"Description\"].str.startswith(\"Clst_\")) &\n",
    "                     (df[\"Description\"].str.contains(\"_O2_\"))].copy(deep = True).reset_index(drop=True)\n",
    "    clustO2[[\"TF\", \"Cluster\", \"Assay\"]] = clustO2[\"Description\"].apply(lambda x: pd.Series(x.split(\"_\")[1:4]))\n",
    "    clustO2[\"Cluster\"] = clustO2[\"Cluster\"].apply(lambda x: int(x[1:]))\n",
    "    m = pd.merge(clustO1, clustO2, left_on = [\"TF\", \"Cluster\", \"Assay\"], right_on = [\"TF\", \"Cluster\", \"Assay\"])\n",
    "    for s, rc in zip(m[\"Sequence_x\"], m[\"Sequence_y\"]):\n",
    "        if s[:25] != reverse_complement(rc[:25]):\n",
    "            raise ValueError(f\"{s} was described as the reverse complement of {rc}\")       \n",
    "\n",
    "def testClusterDistanceDescription(df, testSeq):\n",
    "    clust = df[(df[\"Description\"].str.startswith(\"ClstD_\"))].copy(deep = True).reset_index(drop=True)\n",
    "    clust[\"SubHamming\"] = clust[\"Description\"].apply(lambda x: int(x.split('_')[2][1:]))\n",
    "    for seq, hamDist in zip(clust[\"Sequence\"], clust[\"SubHamming\"]):\n",
    "        distance1 = closestSubHamming(seq[:25], testSeq)\n",
    "        distance2 = closestSubHamming(reverse_complement(seq[:25]), testSeq)\n",
    "        if min([distance1, distance2]) != hamDist:\n",
    "            raise ValueError(f\"Minimum Sub-Hamming Distance does not match description for {seq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
